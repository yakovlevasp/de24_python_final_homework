# Проект построения витрин заказов с использованием Airflow и Spark

## Описание проекта

Этот проект ориентирован на автоматизацию построения аналитических витрин данных, связанных с заказами пользователей, с использованием Apache Airflow и Apache Spark. В рамках проекта создаются несколько витрин для анализа продаж по продуктам и заказам пользователей.

## Архитектура

Проект состоит из двух основных компонентов:

- **Apache Airflow**: Используется для оркестрации процессов построения витрин данных. Airflow запускает DAG'и, которые отвечают за выполнение различных задач, таких как извлечение данных, их обработка и загрузка результатов в базу данных.
  
- **Apache Spark**: Обрабатывает большие объемы данных и выполняет сложные вычисления для создания витрин, используя параллельную обработку.

## Структура витрин

### 1. Витрина продаж по продуктам (Product Sales Mart)

Эта витрина агрегирует информацию о продажах продуктов, включая:

- `product_id`: Идентификатор продукта.
- `name`: Название продукта.
- `description`: Описание продукта.
- `category_id`: Идентификатор категории продукта.
- `category_name`: Название категории продукта.
- `price`: Цена продукта.
- `total_sales`: Сумма продаж продукта.
- `total_orders`: Количество заказов, включающих этот продукт.
- `avg_order_value`: Средняя стоимость заказа, включающего продукт.
- `max_order_value`: Максимальная стоимость заказа с этим продуктом.

Задача `build_product_sales_mart` извлекает данные из таблиц `order_details`, `products`, `product_categories` и объединяет их для вычисления этих метрик.

### 2. Витрина заказов пользователей (User Orders Mart)

Эта витрина агрегирует информацию о заказах пользователей, включая:

- `user_id`: Идентификатор пользователя.
- `first_name`: Имя пользователя.
- `last_name`: Фамилия пользователя.
- `email`: Email пользователя.
- `phone`: Телефон пользователя.
- `total_orders`: Количество заказов пользователя.
- `total_spent`: Общая сумма покупок пользователя.
- `favorite_category`: Самая покупаемая категория товара пользователем.
- `most_purchased_product_count`: Количество наиболее покупаемого товара пользователем.
- `highest_spent_category`: Категория с максимальной потраченной суммой.
- `avg_order_value`: Средняя стоимость заказа пользователя.
- `max_order_value`: Максимальная стоимость одного заказа пользователя.

Задача `build_user_orders_mart` извлекает данные из таблиц `orders`, `users`, `order_details`, `products`, `product_categories` и выполняет необходимые агрегации.


#### Описание DAG'а

Этот DAG **analytics_marts_dag** запускается ежедневно и предназначен для синхронизации данных между PostgreSQL и MySQL. Используется Apache Spark для выполнения репликации данных между двумя базами.

Параметры DAG:
- **Описание**: Репликация данных по заказам из PostgreSQL в MySQL
- **Частота запуска**: Ежедневно
- **Дата начала**: 1 января 2024 года

Для репликации используется Spark, который подключается к обеим базам данных через JDBC и выполняет загрузку данных в MySQL в режиме перезаписи (`overwrite`).

## Описание DAG'ов

### 1. **DAG для создания витрин** (`analytics_marts_dag`)

Этот DAG управляет процессом построения витрин данных. Он состоит из двух задач:
- **build_product_sales_mart**: Построение витрины по продуктам.
- **build_user_orders_mart**: Построение витрины по пользователям.

DAG настроен на ежедневный запуск в 1:00 ночи с помощью планировщика Airflow. Каждая задача выполняет агрегацию и записывает результат в базу данных MySQL.

### 2. **DAG для генерации данных заказов** (`create_orders_data`)

Этот DAG используется для создания фейковых данных, которые будут использоваться для тестирования витрин. Он выполняет следующие задачи:
- **create_tables**: Создает таблицы в базе данных PostgreSQL, если они еще не существуют.
- **populate_tables**: Генерирует случайные данные для пользователей, продуктов, заказов и деталей заказов.
- **trigger_replication**: Запускает репликацию данных в MySQL для дальнейшего использования в витринах.

Этот процесс генерирует фейковые данные для всех необходимых таблиц и может быть полезен для тестирования и разработки.

### 3. **DAG для репликации данных из PostgreSQL в MySQL** (`replicate_postgres_to_mysql`)

Этот DAG управляет процессом репликации данных из PostgreSQL в MySQL. Он выполняет задачу `replicate_data`, которая читает данные из таблиц PostgreSQL и записывает их в MySQL.

Для репликации используется Spark, который подключается к обеим базам данных через JDBC и выполняет загрузку данных в MySQL в режиме перезаписи (`overwrite`).